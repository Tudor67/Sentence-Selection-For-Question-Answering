{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "1. [Loading preprocessed data](#1.-Loading-preprocessed-data)  \n",
    "2. [LR using pretrained word embeddings](#2.-LR-using-pretrained-word-embeddings)  \n",
    "3. [BM25](#3.-BM25)\n",
    "4. [String kernels](#4.-String-kernels)\n",
    "5. [Classification {combined methods}](#5.-Classification-{combined-methods})\n",
    "6. [Evaluation](#6.-Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Loading preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import evaluation\n",
    "import json\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from input_output import load_data, write_results\n",
    "from pprint import pprint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "\n",
    "IN_PATH = '../data/squad/'\n",
    "WORD2VEC_PATH = '../word_embeddings/GoogleNews-vectors-negative300.bin'\n",
    "word2vec_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = load_data(IN_PATH + 'train-v1.1-preprocessed.json')\n",
    "dev = load_data(IN_PATH + 'dev-v1.1-preprocessed.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LR using pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pretrained_word_embeddings(sentence, word_emb_dims=300):\n",
    "    global word2vec_model\n",
    "    \n",
    "    if word2vec_model is None:\n",
    "        print('load word2vec_model')\n",
    "        word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(WORD2VEC_PATH, binary=True)\n",
    "\n",
    "    word_embeddings = np.zeros(word_emb_dims)\n",
    "    word_emb_nr = 0\n",
    "\n",
    "    for word in sentence:\n",
    "        if word2vec_model.vocab.has_key(word):\n",
    "            word_embeddings += word2vec_model.word_vec(word)\n",
    "            word_emb_nr += 1\n",
    "\n",
    "    # a sentence is represented by the average of word_embbedings\n",
    "    if word_emb_nr != 0:\n",
    "        word_embeddings = word_embeddings / word_emb_nr\n",
    "\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(dataset, q_a_combination_method):\n",
    "    train_x = list()\n",
    "    train_y = list()\n",
    "    train_steps = list()\n",
    "    train_labels = list()\n",
    "    \n",
    "    for article in tqdm(dataset['data']):\n",
    "        for qas_context in article['paragraphs']:\n",
    "            for qas in qas_context['qas']:\n",
    "                question_words = qas['question_lemmas_without_stopwords']\n",
    "                question_embedding = get_pretrained_word_embeddings(question_words)\n",
    "                \n",
    "                candidate_sentences_embeddings = list()\n",
    "                for sentence in qas_context['context_sentences_lemmas_without_stopwords']:\n",
    "                    candidate_sentences_embeddings.append(get_pretrained_word_embeddings(sentence))\n",
    "                \n",
    "                labels = set([d['answer_label'] for d in qas['answers']])\n",
    "                train_labels.append(labels)\n",
    "                \n",
    "                train_steps.append(len(candidate_sentences_embeddings))\n",
    "                \n",
    "                for i in range(len(candidate_sentences_embeddings)):\n",
    "                    feature_vector = []\n",
    "                    if q_a_combination_method == 'concatenation':\n",
    "                        feature_vector = np.concatenate((question_embedding,\n",
    "                                                         candidate_sentences_embeddings[i]))\n",
    "                    elif q_a_combination_method == 'diff_abs':\n",
    "                        feature_vector = np.abs(question_embedding - \n",
    "                                                candidate_sentences_embeddings[i])\n",
    "                    elif q_a_combination_method == 'diff_sqr':\n",
    "                        feature_vector = np.square(question_embedding - \n",
    "                                                   candidate_sentences_embeddings[i])\n",
    "                    elif q_a_combination_method == 'sum':\n",
    "                        feature_vector = question_embedding + candidate_sentences_embeddings[i]\n",
    "                    elif q_a_combination_method == 'dot_product':\n",
    "                        feature_vector = question_embedding * candidate_sentences_embeddings[i]\n",
    "                    elif q_a_combination_method == 'min':\n",
    "                        feature_vector = np.minimum(question_embedding,\n",
    "                                                    candidate_sentences_embeddings[i])            \n",
    "                    elif q_a_combination_method == 'max':\n",
    "                        feature_vector = np.maximum(question_embedding,\n",
    "                                                    candidate_sentences_embeddings[i])  \n",
    "                        \n",
    "                    train_x.append(feature_vector)\n",
    "                    label = i in labels\n",
    "                    train_y.append(label)\n",
    "                    \n",
    "    return (train_x, train_y, train_steps, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load word2vec_model\n"
     ]
    }
   ],
   "source": [
    "get_pretrained_word_embeddings(['hello', 'world']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(train_x, train_y, test_x):\n",
    "    pred = np.zeros(len(test_x))\n",
    "\n",
    "    lr_model = LogisticRegression(C=1, dual=True)\n",
    "    lr_model.fit(train_x, train_y)\n",
    "    pred = lr_model.predict_proba(test_x)[:, 1]\n",
    "\n",
    "    return (lr_model, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_scores_for_each_question(y_pred, steps_lengths):\n",
    "    pred_scores = list()\n",
    "    steps_lengths_sum = 0\n",
    "    \n",
    "    for step_length in steps_lengths:\n",
    "        scores = list()\n",
    "        for idx in range(step_length):\n",
    "            scores.append(y_pred[steps_lengths_sum + idx])\n",
    "        \n",
    "        pred_scores.append(scores)\n",
    "        \n",
    "        steps_lengths_sum += step_length\n",
    "        \n",
    "    return pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_LR_and_save_pred_scores(q_a_combination_method, data):\n",
    "    data['LR'] = {'dev_preds': {}, 'train_preds': {}}\n",
    "    iteration = 0\n",
    "    \n",
    "    for q_a_comb in q_a_combination_method:\n",
    "        iteration += 1\n",
    "        # feature extraction\n",
    "        (train_x, train_y, train_steps, train_labels) = get_features(train, q_a_comb)\n",
    "        (dev_x, dev_y, dev_steps, dev_labels) = get_features(dev, q_a_comb)\n",
    "        print(q_a_comb, len(train_x[0]))\n",
    "        \n",
    "        # dev set\n",
    "        (lr_model, dev_preds) = logistic_regression(train_x, train_y, dev_x)\n",
    "        #dev_pred_scores = split_scores_for_each_question(dev_preds, dev_steps)\n",
    "        \n",
    "        # train set\n",
    "        train_preds = lr_model.predict_proba(train_x)[:, 1]\n",
    "        \n",
    "        if iteration == 1:\n",
    "            data['train_steps'] = train_steps\n",
    "            data['dev_steps'] = dev_steps\n",
    "            data['train_labels'] = train_labels\n",
    "            data['dev_labels'] = dev_labels\n",
    "            data['train_y'] = train_y\n",
    "            data['dev_y'] = dev_y\n",
    "            \n",
    "        data['LR']['dev_preds'][q_a_comb] = dev_preds\n",
    "        data['LR']['train_preds'][q_a_comb] = train_preds\n",
    "    \n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { \n",
    "        'LR': {}, 'BM25': {}, 'string_kernels': {}, \n",
    "        'dev_steps': [], 'train_steps': [],\n",
    "        'dev_labels': [], 'train_labels': [],\n",
    "        'dev_y': [], 'train_y': []\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:18<00:00, 23.46it/s]\n",
      "100%|██████████| 48/48 [00:02<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('diff_abs', 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:19<00:00, 22.97it/s]\n",
      "100%|██████████| 48/48 [00:02<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('diff_sqr', 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:18<00:00, 24.04it/s]\n",
      "100%|██████████| 48/48 [00:02<00:00, 21.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dot_product', 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:18<00:00, 23.72it/s]\n",
      "100%|██████████| 48/48 [00:02<00:00, 20.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('min', 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:18<00:00, 23.77it/s]\n",
      "100%|██████████| 48/48 [00:02<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('max', 300)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "q_a_combination_method = ['diff_abs', 'diff_sqr', \n",
    "                          'dot_product', 'min', 'max']\n",
    "\n",
    "run_LR_and_save_pred_scores(q_a_combination_method, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['LR'];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.summarization.bm25 import BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bm25_individual_scores(dataset):\n",
    "    bm25_scores = list()\n",
    "    \n",
    "    for article in tqdm(dataset['data']):\n",
    "        for qas_context in article['paragraphs']:\n",
    "            for qas in qas_context['qas']:\n",
    "                question_sentence = qas['question_lemmas_without_stopwords']\n",
    "                candidate_sentences = qas_context['context_sentences_lemmas_without_stopwords']   \n",
    "                \n",
    "                bm25 = BM25(candidate_sentences)\n",
    "                candidate_scores = bm25.get_scores(question_sentence, 1)\n",
    "                bm25_scores += candidate_scores\n",
    "    \n",
    "    return bm25_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 52.18it/s]\n",
      "100%|██████████| 442/442 [00:07<00:00, 59.68it/s]\n"
     ]
    }
   ],
   "source": [
    "data['BM25'] = {'dev_preds': [], 'train_preds': []}\n",
    "data['BM25']['dev_preds'] = get_bm25_individual_scores(dev)\n",
    "data['BM25']['train_preds'] = get_bm25_individual_scores(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['BM25'];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. String kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string_kernels import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_scores_for_a_given_question(question_words, context_sentences, kernel_type):\n",
    "    scores = []\n",
    "    \n",
    "    for candidate_sentence in context_sentences:\n",
    "        kernel_value = 0\n",
    "        if kernel_type == 'spectrum_kernel':\n",
    "            kernel_value = spectrum_kernel_value(question_words, candidate_sentence)\n",
    "        elif kernel_type == 'presence_kernel':\n",
    "            kernel_value = presence_kernel_value(question_words, candidate_sentence)\n",
    "        elif kernel_type == 'intersection_kernel':\n",
    "            kernel_value = intersection_kernel_value(question_words, candidate_sentence)\n",
    "        \n",
    "        scores.append(kernel_value)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_kernel_scores(dataset, method):\n",
    "    kernel_scores = list()\n",
    "    \n",
    "    for article in dataset['data']:\n",
    "        for qas_context in article['paragraphs']:\n",
    "            for qas in qas_context['qas']:\n",
    "                # trying to keep the same notation\n",
    "                question_words = qas['question_lemmas_without_stopwords']\n",
    "                candidate_answers_words = qas_context['context_sentences_lemmas_without_stopwords']\n",
    "                \n",
    "                # run a method\n",
    "                kernel_scores += kernel_scores_for_a_given_question(question_words, \n",
    "                                                                    candidate_answers_words,\n",
    "                                                                    method)\n",
    "    \n",
    "    return kernel_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['string_kernels'] = {'dev_preds': {}, 'train_preds': {}}\n",
    "data['string_kernels']['dev_preds']['presence_kernel'] = get_kernel_scores(dev, 'presence_kernel')\n",
    "data['string_kernels']['train_preds']['presence_kernel'] = get_kernel_scores(train, 'presence_kernel')\n",
    "data['string_kernels']['dev_preds']['intersection_kernel'] = get_kernel_scores(dev, 'intersection_kernel')\n",
    "data['string_kernels']['train_preds']['intersection_kernel'] = get_kernel_scores(train, 'intersection_kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['string_kernels'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    " \n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "\n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    return None\n",
    "\n",
    " \n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        similarities = [synset.path_similarity(ss) for ss in synsets2]\n",
    "        best_score = 0\n",
    "        if similarities:\n",
    "            best_score = max(similarities)\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    " \n",
    "    # Average the values\n",
    "    if count > 0:\n",
    "        score /= count\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "def symmetric_sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_similarity(sentence1, sentence2) + sentence_similarity(sentence2, sentence1)) / 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_similarity_scores(question_words, candidate_answers_words):\n",
    "    question_sentence = \" \".join(question_words)\n",
    "    scores = list()\n",
    "    \n",
    "    for sentence_label in range(len(candidate_answers_words)):\n",
    "        candidate_answer_sentence = \" \".join(candidate_answers_words[sentence_label])\n",
    "        similarity = symmetric_sentence_similarity(question_sentence, candidate_answer_sentence)\n",
    "        scores.append(similarity)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_similarity_scores(dataset, method):\n",
    "    sentence_similarity_scores = list()\n",
    "    \n",
    "    for article in dataset['data']:\n",
    "        for qas_context in article['paragraphs']:\n",
    "            for qas in qas_context['qas']:\n",
    "                # trying to keep the same notation\n",
    "                question_words = qas['question_lemmas_without_stopwords']\n",
    "                candidate_answers_words = qas_context['context_sentences_lemmas_without_stopwords']\n",
    "                \n",
    "                # run a method\n",
    "                sentence_similarity_scores += sentence_similarity_scores(question_words,\n",
    "                                                                         candidate_answers_words)\n",
    "    \n",
    "    return sentence_similarity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification {combined methods}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_combined = np.vstack((data['LR']['train_preds']['diff_abs'],\n",
    "                              data['LR']['train_preds']['diff_sqr'],\n",
    "                              data['LR']['train_preds']['dot_product'],\n",
    "                              data['LR']['train_preds']['min'],\n",
    "                              data['LR']['train_preds']['max'],\n",
    "                              data['BM25']['train_preds'],\n",
    "                              data['string_kernels']['train_preds']['presence_kernel'],\n",
    "                              data['string_kernels']['train_preds']['intersection_kernel'])).T\n",
    "train_y_combined = data['train_y']\n",
    "train_steps_combined = data['train_steps']\n",
    "train_labels_combined = data['train_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_x_combined = np.vstack((data['LR']['dev_preds']['diff_abs'],\n",
    "                            data['LR']['dev_preds']['diff_sqr'],\n",
    "                            data['LR']['dev_preds']['dot_product'],\n",
    "                            data['LR']['dev_preds']['min'],\n",
    "                            data['LR']['dev_preds']['max'],\n",
    "                            data['BM25']['dev_preds'],\n",
    "                            data['string_kernels']['dev_preds']['presence_kernel'],\n",
    "                            data['string_kernels']['dev_preds']['intersection_kernel'])).T\n",
    "dev_y_combined = data['dev_y']\n",
    "dev_steps_combined = data['dev_steps']\n",
    "dev_labels_combined = data['dev_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_scores(y_pred, steps_lengths):\n",
    "    pred_scores_id = list()\n",
    "    steps_lengths_sum = 0\n",
    "    \n",
    "    for step_length in steps_lengths:\n",
    "        scores = {}\n",
    "        for idx in range(step_length):\n",
    "            scores[idx] = y_pred[steps_lengths_sum + idx]\n",
    "        \n",
    "        sorted_scores_id = sorted(scores, key=scores.get, reverse=True)\n",
    "        pred_scores_id.append(sorted_scores_id)\n",
    "        \n",
    "        steps_lengths_sum += step_length\n",
    "        \n",
    "    return pred_scores_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: dev combined\n",
      "AvgPrec@1: 0.83793755913 (std = 0.368508081498)\n",
      "MAP: 0.898180673612 (std = 0.219387619495)\n",
      "\n",
      "\n",
      "Method: train combined\n",
      "AvgPrec@1: 0.804164431101 (std = 0.396842536597)\n",
      "MAP: 0.882941739795 (std = 0.243246791108)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dev set\n",
    "(lr_model, dev_preds) = logistic_regression(train_x_combined,\n",
    "                                            train_y_combined,\n",
    "                                            dev_x_combined)\n",
    "dev_pred_scores_id = extract_scores(dev_preds, dev_steps_combined)\n",
    "\n",
    "dev_results = evaluation.get_results(dev_pred_scores_id, \n",
    "                                     dev_labels_combined, \n",
    "                                     'dev combined')\n",
    "write_results(dev_results)\n",
    "\n",
    "# train set, just for curiosity\n",
    "train_preds = lr_model.predict_proba(train_x_combined)[:, 1]\n",
    "train_pred_scores_id = extract_scores(train_preds, \n",
    "                                      train_steps_combined)\n",
    "\n",
    "train_results = evaluation.get_results(train_pred_scores_id, \n",
    "                                       train_labels_combined, \n",
    "                                       'train combined')\n",
    "write_results(train_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
