{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "1. [Data loading](#1.-Data-loading)  \n",
    "2. [Structure of the data](#2.-Structure-of-the-data)  \n",
    "3. [Data preprocessing](#3.-Data-preprocessing)  \n",
    "    3.1 [Context/passage sentence tokenization](#3.1-Context/passage-sentence-tokenization-[1])  \n",
    "    3.2 [Word tokenization](#3.2-Word-tokenization-[1])  \n",
    "    3.3 [Lemmatization](#3.3-Lemmatization-[2])  \n",
    "    3.4 [Stopwords elimination](#3.4-Stopwords-elimination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "IN_PATH = '../data/squad/'\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    data = []\n",
    "    with open(filename) as f:\n",
    "        data = json.load(f)  \n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "train = load_data(IN_PATH + 'train-v1.1.json')\n",
    "dev = load_data(IN_PATH + 'dev-v1.1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Structure of the data \n",
    "** The structure of the data can be seen in the following way:  **\n",
    "<pre>       \n",
    "            dataset   \n",
    "            (dict)   \n",
    "            /    \\  \n",
    "           /      \\  \n",
    "      'version'   'data'\n",
    "      (string)    (list)\n",
    "                  # each element in the list represents an article (dict)  \n",
    "                     |\n",
    "                     |\n",
    "                  articles\n",
    "                   (dict)\n",
    "                   /    \\\n",
    "                  /      \\\n",
    "             'title'   'paragraphs'  \n",
    "             (string)     (list)\n",
    "                          # each element in the list is an dict with 'qas' and 'context' keys\n",
    "                             |\n",
    "                             |\n",
    "                            pair\n",
    "                           (dict)\n",
    "                          /      \\\n",
    "                         /        \\\n",
    "                        /          \\\n",
    "                       /            \\\n",
    "                 'context'          'qas'\n",
    "                 (string)           (list)\n",
    "                  # context of      # each element in the list is a dict with 3 keys\n",
    "                    the questions      |\n",
    "                    and answers        |\n",
    "                                       |\n",
    "                                 question-answer\n",
    "                                     (dict)\n",
    "                                    /   |   \\\n",
    "                                   /    |    \\\n",
    "                                  /     |     \\\n",
    "                               'id' 'question' 'answers'\n",
    "                             (string)(string)   (list)\n",
    "                                                # each element in the list is an answer dict with \n",
    "                                                'text' of the answer and 'answer_start'\n",
    "                                                  |\n",
    "                                                  |\n",
    "                                                answer\n",
    "                                                (dict)\n",
    "                                                /     \\\n",
    "                                               /       \\\n",
    "                                        'answer_start' 'text'\n",
    "                                            (int)     (string)                                   \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Context/passage sentence tokenization [[1]](#References)\n",
    "** Let's split sentences for all passages. These sentences are candidate answers for the questions from the same paragraph. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def tokenize_sent_from_context(dataset):\n",
    "    for article in dataset['data']:\n",
    "        for qas_context in article['paragraphs']:\n",
    "            qas_context['context_sentences'] = sent_tokenize(qas_context['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.',\n",
      " u'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24\\u201310 to earn their third Super Bowl title.',\n",
      " u\"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\",\n",
      " u'As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.']\n"
     ]
    }
   ],
   "source": [
    "tokenize_sent_from_context(dev)\n",
    "pprint(dev['data'][0]['paragraphs'][0]['context_sentences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Word tokenization [[1]](#References)\n",
    "** Word tokenization for questions and context with removal of non-alphanumeric tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "def remove_non_alnum(word_list):\n",
    "    return [word.lower() for word in word_list if word.isalnum()]\n",
    "\n",
    "\n",
    "def tokenize_words(dataset):\n",
    "    word_punct_tokenizer = WordPunctTokenizer()\n",
    "    for article in dataset['data']:\n",
    "        for qas_context in article['paragraphs']:\n",
    "            # tokenize all context_sentences\n",
    "            qas_context['context_sentences_words'] = list()\n",
    "            for sentence in qas_context['context_sentences']:\n",
    "                word_list = word_punct_tokenizer.tokenize(sentence)\n",
    "                qas_context['context_sentences_words'].append(remove_non_alnum(word_list))\n",
    "            \n",
    "            # tokenize questions\n",
    "            for qas in qas_context['qas']:\n",
    "                question = qas['question']\n",
    "                word_list = word_punct_tokenizer.tokenize(question)\n",
    "                qas['question_words'] = remove_non_alnum(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_words(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season.'\n",
      "[u'super',\n",
      " u'bowl',\n",
      " u'50',\n",
      " u'was',\n",
      " u'an',\n",
      " u'american',\n",
      " u'football',\n",
      " u'game',\n",
      " u'to',\n",
      " u'determine',\n",
      " u'the',\n",
      " u'champion',\n",
      " u'of',\n",
      " u'the',\n",
      " u'national',\n",
      " u'football',\n",
      " u'league',\n",
      " u'nfl',\n",
      " u'for',\n",
      " u'the',\n",
      " u'2015',\n",
      " u'season']\n"
     ]
    }
   ],
   "source": [
    "pprint(dev['data'][0]['paragraphs'][0]['context_sentences'][0])\n",
    "pprint(dev['data'][0]['paragraphs'][0]['context_sentences_words'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'Which NFL team represented the AFC at Super Bowl 50?'\n",
      "[u'which',\n",
      " u'nfl',\n",
      " u'team',\n",
      " u'represented',\n",
      " u'the',\n",
      " u'afc',\n",
      " u'at',\n",
      " u'super',\n",
      " u'bowl',\n",
      " u'50']\n"
     ]
    }
   ],
   "source": [
    "pprint(dev['data'][0]['paragraphs'][0]['qas'][0]['question'])\n",
    "pprint(dev['data'][0]['paragraphs'][0]['qas'][0]['question_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Lemmatization [[2]](2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmas = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def lemmatization(dataset):\n",
    "    word_punct_tokenizer = WordPunctTokenizer()\n",
    "    for article in dataset['data']:\n",
    "        for qas_context in article['paragraphs']:\n",
    "            # lemmatize all context_sentences_words\n",
    "            qas_context['context_sentences_lemmas'] = list()\n",
    "            for word_list in qas_context['context_sentences_words']:\n",
    "                qas_context['context_sentences_lemmas'].append(lemmatize_tokens(word_list))\n",
    "            \n",
    "            # lemmatize questions\n",
    "            for qas in qas_context['qas']:\n",
    "                word_list = qas['question_words']\n",
    "                qas['question_lemmas'] = lemmatize_tokens(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u\"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"\n",
      "[u'the',\n",
      " u'game',\n",
      " u'was',\n",
      " u'played',\n",
      " u'on',\n",
      " u'february',\n",
      " u'7',\n",
      " u'2016',\n",
      " u'at',\n",
      " u'levi',\n",
      " u's',\n",
      " u'stadium',\n",
      " u'in',\n",
      " u'the',\n",
      " u'san',\n",
      " u'francisco',\n",
      " u'bay',\n",
      " u'area',\n",
      " u'at',\n",
      " u'santa',\n",
      " u'clara',\n",
      " u'california']\n",
      "[u'the',\n",
      " u'game',\n",
      " u'be',\n",
      " u'play',\n",
      " u'on',\n",
      " u'february',\n",
      " u'7',\n",
      " u'2016',\n",
      " u'at',\n",
      " u'levi',\n",
      " u's',\n",
      " u'stadium',\n",
      " u'in',\n",
      " u'the',\n",
      " u'san',\n",
      " u'francisco',\n",
      " u'bay',\n",
      " u'area',\n",
      " u'at',\n",
      " u'santa',\n",
      " u'clara',\n",
      " u'california']\n"
     ]
    }
   ],
   "source": [
    "pprint(dev['data'][0]['paragraphs'][0]['context_sentences'][2])\n",
    "pprint(dev['data'][0]['paragraphs'][0]['context_sentences_words'][2])\n",
    "pprint(dev['data'][0]['paragraphs'][0]['context_sentences_lemmas'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'Which NFL team represented the AFC at Super Bowl 50?'\n",
      "[u'which',\n",
      " u'nfl',\n",
      " u'team',\n",
      " u'represented',\n",
      " u'the',\n",
      " u'afc',\n",
      " u'at',\n",
      " u'super',\n",
      " u'bowl',\n",
      " u'50']\n",
      "[u'which',\n",
      " u'nfl',\n",
      " u'team',\n",
      " u'represent',\n",
      " u'the',\n",
      " u'afc',\n",
      " u'at',\n",
      " u'super',\n",
      " u'bowl',\n",
      " u'50']\n"
     ]
    }
   ],
   "source": [
    "pprint(dev['data'][0]['paragraphs'][0]['qas'][0]['question'])\n",
    "pprint(dev['data'][0]['paragraphs'][0]['qas'][0]['question_words'])\n",
    "pprint(dev['data'][0]['paragraphs'][0]['qas'][0]['question_lemmas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Stopwords elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stopwords_set = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in set(stopwords_set)]   \n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def stopwords_elimination(dataset):\n",
    "    for article in dataset['data']:\n",
    "        for qas_context in article['paragraphs']:\n",
    "            # remove stopwords from context_sentences_lemmas\n",
    "            qas_context['context_sentences_lemmas_without_stopwords'] = list()\n",
    "            for lemmas_list in qas_context['context_sentences_lemmas']:\n",
    "                qas_context['context_sentences_lemmas_without_stopwords'].append(remove_stopwords(lemmas_list))\n",
    "            \n",
    "            # remove stopwords from questions lemmas\n",
    "            for qas in qas_context['qas']:\n",
    "                lemmas_list = qas['question_lemmas']\n",
    "                qas['question_lemmas_without_stopwords'] = remove_stopwords(lemmas_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_elimination(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u\"The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"\n",
      "[u'the',\n",
      " u'game',\n",
      " u'was',\n",
      " u'played',\n",
      " u'on',\n",
      " u'february',\n",
      " u'7',\n",
      " u'2016',\n",
      " u'at',\n",
      " u'levi',\n",
      " u's',\n",
      " u'stadium',\n",
      " u'in',\n",
      " u'the',\n",
      " u'san',\n",
      " u'francisco',\n",
      " u'bay',\n",
      " u'area',\n",
      " u'at',\n",
      " u'santa',\n",
      " u'clara',\n",
      " u'california']\n",
      "[u'the',\n",
      " u'game',\n",
      " u'be',\n",
      " u'play',\n",
      " u'on',\n",
      " u'february',\n",
      " u'7',\n",
      " u'2016',\n",
      " u'at',\n",
      " u'levi',\n",
      " u's',\n",
      " u'stadium',\n",
      " u'in',\n",
      " u'the',\n",
      " u'san',\n",
      " u'francisco',\n",
      " u'bay',\n",
      " u'area',\n",
      " u'at',\n",
      " u'santa',\n",
      " u'clara',\n",
      " u'california']\n",
      "[u'game',\n",
      " u'play',\n",
      " u'february',\n",
      " u'7',\n",
      " u'2016',\n",
      " u'levi',\n",
      " u'stadium',\n",
      " u'san',\n",
      " u'francisco',\n",
      " u'bay',\n",
      " u'area',\n",
      " u'santa',\n",
      " u'clara',\n",
      " u'california']\n"
     ]
    }
   ],
   "source": [
    "pprint(dev['data'][0]['paragraphs'][0]['context_sentences'][2])\n",
    "pprint(dev['data'][0]['paragraphs'][0]['context_sentences_words'][2])\n",
    "pprint(dev['data'][0]['paragraphs'][0]['context_sentences_lemmas'][2])\n",
    "pprint(dev['data'][0]['paragraphs'][0]['context_sentences_lemmas_without_stopwords'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'Which NFL team represented the AFC at Super Bowl 50?'\n",
      "[u'which',\n",
      " u'nfl',\n",
      " u'team',\n",
      " u'represented',\n",
      " u'the',\n",
      " u'afc',\n",
      " u'at',\n",
      " u'super',\n",
      " u'bowl',\n",
      " u'50']\n",
      "[u'which',\n",
      " u'nfl',\n",
      " u'team',\n",
      " u'represent',\n",
      " u'the',\n",
      " u'afc',\n",
      " u'at',\n",
      " u'super',\n",
      " u'bowl',\n",
      " u'50']\n",
      "[u'nfl', u'team', u'represent', u'afc', u'super', u'bowl', u'50']\n"
     ]
    }
   ],
   "source": [
    "pprint(dev['data'][0]['paragraphs'][0]['qas'][0]['question'])\n",
    "pprint(dev['data'][0]['paragraphs'][0]['qas'][0]['question_words'])\n",
    "pprint(dev['data'][0]['paragraphs'][0]['qas'][0]['question_lemmas'])\n",
    "pprint(dev['data'][0]['paragraphs'][0]['qas'][0]['question_lemmas_without_stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] http://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize  \n",
    "[2] http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
